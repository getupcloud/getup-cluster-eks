{{- if and .Values.mode.overlay .Values.monitoring.enabled }}
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: monitoring
  namespace: flux-system
spec:
  {{- include "fluxValuesFrom" (dict "secretName" "monitoring" "values" .Values.monitoring.extraValues) | nindent 2 }}
  values:
    ###############################################
    prometheus:
      prometheusSpec:
        {{- if .Values.tempo.enabled }}
        enableRemoteWriteReceiver: true
        remoteWriteDashboards: true
        enableFeatures:
        - exemplar-storage
       {{- end }}

    ###############################################
    grafana:
      sidecar:
        datasources:
          {{- if .Values.tempo.enabled }}
          exemplarTraceIdDestinations:
            datasourceUid: tempo
            traceIdLabelName: traceID
          {{- end }}

      grafana.ini:
        feature_toggles:
          {{- if .Values.tempo.enabled }}
          traceToMetrics: true
          {{- end }}

      additionalDataSources:
        {{- if .Values.tempo.enabled }}
        name: Tempo
        type: tempo
        uid: tempo
        editable: true
        url: http://tempo:3100
        basicAuth: false
        access: proxy
        isDefault: false
        jsonData:
          timeout: 60
          httpMethod: GET
          tracesToLogs:
            datasourceUid: loki
            mapTagNamesEnabled: true
            mappedTags:
            - key: host.name
              value: pod
            spanStartTimeShift: '-15m'
            spanEndTimeShift: '15m'
            filterByTraceID: true
            filterBySpanID: false
          tracesToMetrics:
            datasourceUid: prometheus
            tags:
            - key: host.name
              value: pod
            spanStartTimeShift: '-15m'
            spanEndTimeShift: '15m'
            queries:
            - name: 'Pod CPU'
              query: 'sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate{$$__tags, container!="POD"}) by (container)'
            - name: 'Pod Memory'
              query: 'sum(container_memory_working_set_bytes{$$__tags, job="kubelet", metrics_path="/metrics/cadvisor", container!="POD", image!=""}) by (container)'
          serviceMap:
            datasourceUid: prometheus
          search:
            hide: false
          nodeGraph:
            enabled: true
          lokiSearch:
            datasourceUid: loki
        {{- end }}
        {{- if .Values.loki.enabled }}
        additionalDataSources:
        - name: Loki
          type: loki
          uid: loki
          editable: true
          url: http://loki-gateway.logging.svc.cluster.local.:80
          basicAuth: false
          access: proxy
          isDefault: false
          jsonData:
            maxLines: 5000
            manageAlerts: false
            timeout: 120
            derivedFields:
            - datasourceUid: tempo
              matcherRegex: "traceID=(\\w+)"
              name: TraceID
              url: "$${__value.raw}"
        {{- end }}

    ###############################################
    alertmanager:
      config:
        route:
          #################################################################
          ## Routes
          #################################################################
          routes:
          # watchdog aims to test the alerting pipeline
          - matchers:
            - alertname = Watchdog
            continue: false

          {{- with .Values.monitoring.ignoredAlerts }}
          # Ignore following alerts and/or namespaces
          - receiver: blackhole
            matchers:
            {{- range . }}
            - alertname =~ {{ .name }}
              {{- if .namespace }}
              namespace =~ {{ .namespace }}
              {{- end }}
            {{- end }}
            continue: false
          {{- end }}

          {{- if .Values.monitoring.cronitorPingUrl }}
          #############################
          # Cronitor
          #############################
          - receiver: cronitor
            matchers:
            - alertname = CronitorWatchdog
            group_wait: 1s
            group_interval: 10s
            continue: false
          {{- end }}

          {{- if .Values.monitoring.slackApiUrl }}
          #############################
          # Slack
          #############################
          - receiver: slack
            matchers:
            - alertname != ""
            continue: true
          {{- end }}

          {{- if .Values.monitoring.msteamsChannelUrl }}
          #############################
          # MS Teams
          #############################
          - receiver: msteams
            matchers:
            - alertname != ""
            continue: true
          {{- end }}

          {{- if .Values.monitoring.pagerdutyServiceKey }}
          #############################
          # PageDuty
          #############################
          - receiver: pagerduty
            matchers:
            - alertname != ""
            - severity: critical
            continue: true
          {{- end }}

          {{- if .Values.monitoring.opsgenieIntegrationApiKey }}
          #############################
          # Opsgenie
          #############################
          - receiver: opsgenie
            matchers:
            - alertname =~ KubeNodeUnreachable|KubeCronJobRunning|KubeDaemonSetRolloutStuck|KubePodCrashLooping|KubePodNotReady|KubeStatefulSetGenerationMismatch|KubeStatefulSetReplicasMismatch
            - namespace =~ logging|monitoring|velero|cert-manager|getup|.*-(ingress|controllers?|operators?|provisioners?|system)
            - severity = warning
            continue: true

          - receiver: opsgenie
            matchers:
            - alertname =~ KubeDeploymentGenerationMismatch|KubeDeploymentReplicasMismatch|KubeNodeNotReady|KubeAPILatencyHigh|KubeStatefulSetUpdateNotRolledOut|KubeJobCompletion|KubeJobFailed
            - namespace =~ logging|monitoring|velero|cert-manager|getup|.*-(ingress|controllers?|operators?|provisioners?|system)
            - severity = warning
            continue: true

          - receiver: opsgenie
            matchers:
            - alertname =~ CertificateAlert|KubeClientCertificateExpiration|ClockSkewDetected|EndpointDown|HighNumberOfFailedProposals|HighNumberOfFailedHTTPRequests
            - namespace =~ logging|monitoring|velero|cert-manager|getup|.*-(ingress|controllers?|operators?|provisioners?|system)
            - severity = warning
            continue: true

          - receiver: opsgenie
            matchers:
            - alertname =~ AlertmanagerFailedReload|PrometheusOperatorReconcileErrors|PrometheusConfigReloadFailed|PrometheusNotConnectedToAlertmanagers|PrometheusTSDBReloadsFailing|PrometheusTSDBCompactionsFailing|PrometheusTSDBWALCorruptions|PrometheusNotIngestingSamples
            - namespace = monitoring
            - severity = warning
            continue: true

          - receiver: opsgenie
            matchers:
            - severity = critical
            continue: true
          {{- end }}

          # ignore non-matching alerts. This is mostly for metrics purpose.
          - receiver: blackhole
            matchers:
            - alertname != ""
            continue: false

        #################################################################
        ## Receivers
        ##
        ## SLA must obey the following rules:
        ##
        ## high: prod/homolog/preprod
        ##  low: dev/test
        ## none: no-ops
        #################################################################

        receivers:
        # does nothing
        - name: blackhole

        {{- if .Values.monitoring.cronitorPingUrl }}
        #############################
        # Cronitor
        #############################
        - name: cronitor
          webhook_configs:
          - url: {{ .Values.monitoring.cronitorPingUrl }}
            send_resolved: false
        {{- end }}

        {{- if .Values.monitoring.slackApiUrl }}

        #############################
        # Slack
        #############################
        - name: slack
          slack_configs:
          - send_resolved: true
            api_url: {{ .Values.monitoring.slackApiUrl }}
            {{- if .Values.monitoring.slackChannel }}
            channel: {{ .Values.monitoring.slackChannel }}
            {{- end }}
            {{- (list
            "color: |-"
            "  {{- if eq .Status \"firing\" -}}"
            "    {{- if eq (index .Alerts 0).Labels.severity \"critical\" -}}"
            "      #FF2222"
            "    {{- end -}}"
            "    {{- if eq (index .Alerts 0).Labels.severity \"warning\" -}}"
            "      #FF8800"
            "    {{- end -}}"
            "    {{- if and (ne (index .Alerts 0).Labels.severity \"critical\") (ne (index .Alerts 0).Labels.severity \"warning\") -}}"
            "      #22FF22"
            "    {{- end -}}"
            "  {{- else -}}"
            "    #22FF22"
            "  {{- end -}}"
            "title: '{{ template \"slack.default.title\" . }}'"
            "pretext: '{{ .CommonAnnotations.summary }}'"
            "fallback: '{{ template \"slack.default.fallback\" . }}'"
            "text: |-"
            "  {{ range .Alerts -}}"
            "  *Severity:* `{{ .Labels.severity | title }}` (<{{ .GeneratorURL }}|graph>)"
            "  *Description:* {{ .Annotations.message }}"
            "  *Labels:*{{ range .Labels.SortedPairs }} `{{ .Name }}={{ .Value }}`{{ end }}"
            "  {{ end }}"
            | join "\n") | nindent 12 }}
        {{- end }}

        {{- if .Values.monitoring.msteamsChannelUrl }}
        #############################
        # MSTeams
        #############################
        - name: msteams
          webhook_configs:
          - url: {{ .Values.monitoring.msteamsChannelUrl }}
        {{- end }}

        {{- if .Values.monitoring.opsgenieIntegrationApiKey }}
        #############################
        # Opsgenie
        #############################
        - name: opsgenie
          opsgenie_configs:
          - api_key: {{ .Values.monitoring.opsgenieIntegrationApiKey }}
            tags: {{ .Values.customer.customerName }}, {{ include "clusterName" . }}, {{ .Values.cluster.clusterProvider }}, sla-{{ .Values.customer.clusterSla }}
            # Choose priority according to SLA: high=[P1|P2], low=[P3|P4|P5]
            {{- if eq .Values.customer.clusterSla "high" }}
            priority: P1
            {{- else }}
            priority: P3
            {{- end }}
        {{- end }}

        {{- if .Values.monitoring.pagerdutyServiceKey }}
        #############################
        # PagerDuty
        #############################
        - name: pagerduty
          pagerduty_configs:
          - service_key: {{ .Values.monitoring.pagerdutyServiceKey }}
            group: {{ .Values.customer.clusterSla }}
        {{- end }}
---
{{ include "fluxValuesFromData" (dict "secretName" "monitoring" "values" .Values.monitoring.extraValues) }}
{{- end }}
